{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba08efed-9774-483a-9343-de715f956669",
   "metadata": {},
   "source": [
    "# Insallting llama index packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68913c49-94b9-40c6-82da-aa3da486a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Using cached llama_index-0.11.23-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.23 (from llama-index)\n",
      "  Using cached llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
      "  Using cached llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_readers_file-0.3.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Using cached openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (6.0.2)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached SQLAlchemy-2.0.36-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached aiohttp-3.10.10-cp313-cp313-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.23->llama-index) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.12.0,>=0.11.23->llama-index)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [21 lines of output]\n",
      "  + C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\\vendored-meson\\meson\\meson.py setup C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537 C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\\.mesonpy-ydb3j2_5 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\\.mesonpy-ydb3j2_5\\meson-python-native-file.ini\n",
      "  The Meson build system\n",
      "  Version: 1.2.99\n",
      "  Source dir: C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\n",
      "  Build dir: C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\\.mesonpy-ydb3j2_5\n",
      "  Build type: native build\n",
      "  Project name: NumPy\n",
      "  Project version: 1.26.4\n",
      "  WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "  ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "  The following exception(s) were encountered:\n",
      "  Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "  A full log can be found at C:\\Users\\asus\\AppData\\Local\\Temp\\pip-install-vshesrl_\\numpy_875a401cff044495b0c53596ca7c1537\\.mesonpy-ydb3j2_5\\meson-logs\\meson-log.txt\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (75.4.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached wheel-0.45.0-py3-none-any.whl (72 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip setuptools wheel\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a474780-6301-4a25-9c4d-c2bfe4a01ab6",
   "metadata": {},
   "source": [
    "# Setting up OPEN AI API KEY in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced266f4-f35b-46a2-8538-f877d6959967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"]=\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788ed30-552f-4ddc-89e3-9e4c3e006435",
   "metadata": {},
   "source": [
    "# Setting up original document path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc376e71-9ff3-4ca8-99c2-617fc4589e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path=r\"E:\\Gen Ai\\e.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1fee67-be39-4755-8ffd-86f3067b1a1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDirectoryReader\n\u001b[0;32m      2\u001b[0m reader\u001b[38;5;241m=\u001b[39mSimpleDirectoryReader(input_files\u001b[38;5;241m=\u001b[39m[f_path])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "reader=SimpleDirectoryReader(input_files=[f_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e5632-1ccb-4b81-92a8-2078e975fbd4",
   "metadata": {},
   "source": [
    "# loading documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1c516-7fc5-4902-b8ef-9da20037539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e156ff0-b3d6-4576-902c-9400ba9af1a7",
   "metadata": {},
   "source": [
    "# setting up index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1061021-b6a1-4158-ac8b-8ea655067461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df427fd-17bb-41ea-948a-fb36000fce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfedf9e-7353-4a5f-91ff-fdc4e6a63acb",
   "metadata": {},
   "source": [
    "# setting up query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02f35d-aa9d-4c5b-9f39-f48970e2ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c482f2a9-b42b-4f77-b8b8-aca5f882947d",
   "metadata": {},
   "source": [
    "# asking question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40b2c9-980d-457b-a40c-c95c68664920",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"where did alice go?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8ed33-4b02-43ad-ad25-ae02ca746e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"what are the things Alice explored?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5206f-1b59-464d-a2cf-3ff163a0b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"how did Alice returned\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b57d69-297b-4fb2-9c86-f4b4bfcd42d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
